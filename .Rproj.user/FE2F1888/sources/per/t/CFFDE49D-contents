---
title: "Friends"
author: "Vinod"
date: "July 28, 2019"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(rvest)
library(here)
library(tidyverse)
library(rlist)
library(RCurl)
library(urltools)
library(httr)
library(readtext)
#install.packages('RColorBrewer')
#most of the libraries needed
library(dplyr) #data manipulation
library(ggplot2) #visualizations
library(gridExtra) #viewing multiple plots together
library(tidytext) #text mining
library(wordcloud2) #creative visualizations
library(RColorBrewer)
library(tm)
library(NLP)
library(wordcloud)
library(stringr)


```


```{r}
# extractscript = function(Season, Episode){
# url <- paste0("https://fangj.github.io/friends/season/",Season,Episode,".html")
# friends <- read_html(url)
# script <- friends %>% 
#   html_nodes(xpath = "/html/body") %>%
#   html_text() %>% 
#   as.character()
# script %>% write(here("Data",paste0("FriendsS",Season,"E",Episode,".txt")))
# c <- c(url,script)
# return(url)
# }
# 
# 
# Season <- c(10)
# Season = append(paste0("0",Season[c(which(Season<10))]),Season[Season>=10])
# Season <- Season[-1]
# Episode <- c(1:16)
# Episode = append(paste0("0",Episode[c(which(Episode<10))]),Episode[Episode>=10])
# #Episode = Episode[-1]
# for (i in Season){
#   for (j in Episode){
#     extractscript(i,j)
#   }
# }
# # # ifelse(url("https://fangj.github.io/friends/season/0101.html")==TRUE," ")
# # # v <- url("https://fangj.github.io/friends/season/0101.html")
# # #try(url.exists("https://www.compassred.com/",.header = FALSE))
# # getURLContent("https://fangj.github.io/friends/season/0105.html")
# # b = url("https://fangj.github.io/friends/season/0105.html")
# # grepl(str(b), "'url' int 6")
# # example_url <- "https://fangj.github.io/friends/season/0125.html"
# # param_get(example_url)
# # domain_name <- url_parse("https://fangj.github.io/friends/season/0101.html")$domain
# # GET("https://fangj.github.io/friends/season/0101.html")
# # url.exists("https://fangj.github.io/friends/season/0101.html", useragent="curl/7.39.0 Rcurl/1.95.4.5")
# # url_success("https://fangj.github.io/friends/season/0101.html")

```

```{r}
myFiles <- list.files(path = "C://Users//Vinod//Desktop//Multivariate Analysis//Friends//Data", pattern = "\\.txt$")



title <- gsub("\\..*","",myFiles)
h <- c()
for (i in title){
  print(i)
  readtxt <- read_csv(here("Data",paste0(i,".txt")))
  h <- append(h,readtxt,after = length(h))
  
}

league <- setNames(as.list(h), title)
#colnames(as.data.frame(league[228]))

```


```{r}
Speakers <- matrix(ncol=3,nrow=1)
#Speakers$Name <- as.character(Speakers$Name)


progress<- c()
for (j in league){
  
progress <- append(progress,".")  
length <- length(progress)
print(length(progress))
if (length>=1 & length <= 24){
  Season = "Season 1"
}
if (length>=25 & length <= 47){
  Season <- "Season 2"
}
if (length>=48 & length <= 72){
  Season <- "Season 3"
}
if (length>=73 & length <= 95){
  Season <- "Season 4"
}
if (length>=96 & length <= 118){
  Season <- "Season 5"
}
if (length>=119 & length <= 141){
  Season <- "Season 6"
}
if (length>=142 & length <= 165){
  Season <- "Season 7"
}
if (length >= 166 & length <= 188){
  Season <- "Season 8"
}
if (length>=189 & length <= 211){
  Season <- "Season 9"
}
if (length>=212 & length <= 228){
  Season <- "Season 10"
}

sc<- as.data.frame(j)
sc[,1] <- as.character(sc[,1])


print("1")

for (i in (sc[,1])){
  check = str_detect(i,":")
  rown <- which(sc[,1] %in% i)
  if (check == FALSE){
    sc[rown-1, ] <- paste(sc[rown-1, ],".", sc[rown, ])
  }
  
}
library(tools)
print("2")
sc <- sc %>% 
  slice(-c(which(str_detect(.[,1],":")==FALSE))) %>% 
  mutate(Text = as.factor(.[,1])) %>% 
  select(Text) %>% 
  separate(Text,into = c("Name","Dialogue"), sep = ":", remove = TRUE, convert = FALSE, extra = "merge") 
print("3")

bind <- cbind(sc$Name,Season,sc$Dialogue)

Speakers <- rbind(Speakers,bind)
}

Speaker <- Speakers[-1,]
Speaker <- as.data.frame(Speaker)
colnames(Speaker) <- c("Name","Season","Dialogue")


```




```{r}
fix.contractions <- function(doc) {
  doc <- gsub("won't", "will not", doc)
  doc <- gsub("can't", "can not", doc)
  doc <- gsub("n't", " not", doc)
  doc <- gsub("'ll", " will", doc)
  doc <- gsub("'re", " are", doc)
  doc <- gsub("'ve", " have", doc)
  doc <- gsub("'m", " am", doc)
  doc <- gsub("'d", " would", doc)
  doc <- gsub("'s", " is", doc)
  doc <- gsub("C'mon", "Come on", doc)
  return(doc)
}

removeSpecialChars <- function(x) gsub("[^a-zA-Z0-9 ]", " ", x)




Speakerd <- as.data.frame(sapply(Speaker, fix.contractions))
Speakerd <- as.data.frame(sapply(Speakerd, removeSpecialChars))
Speakerd <- as.data.frame(sapply(Speakerd, tolower))
Speakerd$Dialogue <- as.character(Speakerd$Dialogue)

```

# Number of times spoken by each character

```{r}

Number_Spoken <-Speakerd %>% 
group_by(Name,Season) %>% 
  count(Name) %>%
  arrange(desc(n)) %>%
  ungroup() %>% 
  mutate(Count = n) %>% 
  select(-n) %>% 
  filter(Name == "monica"|
           Name == "joey"|
           Name == "chandler"|
           Name == "phoebe"|
           Name == "ross"|
           Name == "rachel") %>% 
  mutate(Name = fct_reorder(Name,Count)) %>% 
  ggplot(aes(x=Name,
             y=Count,
             fill = Name,
             label = Count))+
  geom_bar(stat = "identity")+
  geom_text(position = "stack", hjust = 0.5, vjust =1 )+
  theme(legend.position = "none")+
  labs(title = "Number of times spoken by each character",
       xlab = "Characters",
       ylab = "Number of times spoken")+
  facet_wrap(~ Season,nrow = 5,"free_y")


Number_Spoken
```

```{r}

Speakerd %>% 
  mutate(n_words = str_count(Dialogue)) %>% 
  group_by(Name,Season) %>% 
  summarise(n_words = sum(n_words)) %>% 
  arrange(desc(n_words)) %>% 
  filter(Name == "monica"|
           Name == "joey"|
           Name == "chandler"|
           Name == "phoebe"|
           Name == "ross"|
           Name == "rachel") %>% 
  ungroup() %>% 
  mutate(Name = fct_reorder(Name,n_words)) %>% 
  ggplot(aes(x=Name,
             y=n_words,
             fill = Name,
             label = n_words))+
  geom_bar(stat = "identity")+
  geom_text(position = "stack", hjust = 0.5, vjust =1 )+
  theme(legend.position = "none")+
  labs(title = "Word Count",
       xlab = "Characters",
       ylab = "Count of words")+
  facet_wrap(~ Season,nrow = 5,"free_y")
```


```{r}

Speakerd %>% 
  mutate(n_words = str_count(Dialogue)) %>% 
  group_by(Name,Season) %>% 
  mutate(n_words = sum(n_words)) %>% 
  add_count(Name) %>% 
  arrange(desc(n_words),desc(n)) %>% 
  filter(Name == "monica"|
           Name == "joey"|
           Name == "chandler"|
           Name == "phoebe"|
           Name == "ross"|
           Name == "rachel") %>% 
  ggplot(aes(x=n_words,
             y=n,
             label = Name))+
  geom_point()+
  geom_text(size = 3, check_overlap = TRUE)+
  facet_wrap(~ Season,nrow = 5,"free_y")+
  labs(title = "Dialogue Delivery",
       x = "Number of words",
       y = "Number of Times Spoke")



```




```{r}
sf <- Speakerd %>% 
  filter(Name == "monica"|
           Name == "joey"|
           Name == "chandler"|
           Name == "phoebe"|
           Name == "ross") 

  

sf <- sf %>% 
  separate(Dialogue,into = c(as.character(1:max(sapply(strsplit(sf$Dialogue, " "), length)))), sep = " ", remove = TRUE, convert = FALSE, fill = "right")
  # sf[c(which(sf=="")),] <- NA




class(sf$`1`[1])



```

```{r}


names(amazon_review) <- c("Date", "Review")
with_reviews <- length(which(complete.cases(amazon_review)))
with_reviews
without_reviews <- length(which(!complete.cases(amazon_review)))
without_reviews
amazon_text <- paste(amazon_review$Review, collapse=" ")

amazon_source <- VectorSource(amazon_text)
amazon_corpus <- VCorpus(amazon_source)
amazon_corpus <- tm_map(amazon_corpus, content_transformer(tolower))
amazon_corpus <- tm_map(amazon_corpus, removePunctuation)
amazon_corpus <- tm_map(amazon_corpus,stripWhitespace)
amazon_corpus <- tm_map(amazon_corpus, removeWords, stopwords("english"))
dtm <- DocumentTermMatrix(amazon_corpus)
dtm2 <- as.matrix(dtm)
v <- sort(colSums(dtm2), decreasing = TRUE )
d <- data.frame(word = names(v), freq=v)


split1 <- str_split(v, pattern = "\\s+")
class(split1)
split1 <- unlist(split1)

str(split1)

poswords <- scan('pos.txt',what='character', comment.char=";")
#a <- lapply(v, function(v) poswords[match(poswords, split1)])
#b <- str_extract(v, paste(poswords, collapse = "|"))


str(poswords)
negwords <- scan('neg.txt',what='character', comment.char=";")
str(negwords)
positive <- match(split1, poswords)
positive <- str_extract(split1, paste(positive, collapse = "|"))
negative <- match(split1, negwords)
positive

#contains(split1, poswords)
sum(is.na(positive))
sum(is.na(Negative))
wordlist <- sort(colSums(dtm2))
word = names(v)
freq=v
wordcloud(word[1:100], freq[1:100])
barplot(d[5:15,]$freq, las = 2, names.arg = d[5:15,]$word,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")

setwd("C:\\Users\\Vinod\\Desktop\\MIS\\Project\\Dataset")
amazon_review <- read.csv("amazon.csv", stringsAsFactors = FALSE)
names(amazon_review) <- c("Date", "Review")
#amazon_text <- paste(amazon_review$Review, collapse=" ")
amazon_text <- c(amazon_review$Review)
#amazon_text <- str_split(amazon_review$Review, pattern = "\\s+")
class(amazon_text)
amazon_text<-as.list(amazon_text)
class(amazon_text)
#amazon_text <- as.matrix(amazon_text)

sentiments

#for (i in 1:5){
# afinn <- get_sentiments("afinn")
#bing = data.frame(bing$word,bing$sentiment)
#afinn <- as.matrix(afinn)
#afinnpos <- c(which(afinn[,2]<0))
#afinnposwords <- afinn[(afinnpos),1]

#amazon_text <- as.list(str_split(amazon_text[i], pattern = "\\s+"))
#amazon_source <- VectorSource(amazon_text)
#amazon_corpus <- VCorpus(amazon_source)
#match <- intersect(amazon_corpus,afinnposwords)
#next()
#}
#grep(afinnposwords, amazon_text[1])
#intersect(amazon_text, bingposwords)
#bingposwords <- paste(bingposwords, collapse=" ")
#bingposwords <- str_split(bingposwords, pattern = "\\s+")
bingneg <- c(which(bing[,2]=="negative"))
bingnegwords <- bing[(bingneg),1]
length(bingposwords)

bing <- get_sentiments("bing")
#bing = data.frame(bing$word,bing$sentiment)
bing <- as.matrix(bing)
bingpos <- c(which(bing[,2]=="positive"))
bingposwords <- bing[(bingpos),1]
#bingposwords <- paste(bingposwords, collapse=" ")
#bingposwords <- str_split(bingposwords, pattern = "\\s+")
bingneg <- c(which(bing[,2]=="negative"))
bingnegwords <- bing[(bingneg),1]
length(bingposwords)

nrc <- get_sentiments("nrc")
#nrc = data.frame(nrc$word,nrc$sentiment)
nrc <- as.matrix(nrc)
nrcpos <- c(which((nrc[,2]=="positive")|(nrc[,2]=="trust")|(nrc[,2]=="joy")))
nrcposwords <- nrc[(nrcpos),1]
nrcneg <- c(which((nrc[,2]=="negative")|(nrc[,2]=="fear")|(nrc[,2]=="anger")|(nrc[,2]=="disgust")|(nrc[,2]=="sadness")))
nrcnegwords <- nrc[(nrcneg),1]


amazon_source <- VectorSource(amazon_text)
amazon_corpus <- VCorpus(amazon_source)
amazon_corpus <- tm_map(amazon_corpus, content_transformer(tolower))
amazon_corpus <- tm_map(amazon_corpus, removePunctuation)
amazon_corpus <- tm_map(amazon_corpus,stripWhitespace)
amazon_corpus <- tm_map(amazon_corpus, removeWords, stopwords("english"))
dtm <- DocumentTermMatrix(amazon_corpus)
dtm2 <- as.matrix(dtm)
amazon_words <- sort(colSums(dtm2), decreasing = TRUE )
frequency <- data.frame(word = names(amazon_words), freq=amazon_words)
class(amazon_text)
class(bingposwords)
class(bingnegwords)
poswords <- scan('pos.txt',what='character', comment.char=";")
class(poswords)

a<-c(intersect(amazon_text, bingposwords))
a
b<-c(intersect(amazon_text, bingnegwords))
b
c<-c(intersect(amazon_text, nrcposwords))
c
d<-c(intersect(amazon_text, nrcnegwords))
d

positive_words <- c(a, c)
positive_words
negative_words <- c(b, d)
negative_words
sum(!is.na(match (amazon_text, bingposwords)))
sum(!is.na(match (amazon_text, nrcposwords)))
sum(!is.na(match (amazon_text, bingnegwords)))
sum(!is.na(match (amazon_text, nrcnegwords)))



#wordcloud(word[1:100], freq[1:100])
wordcloud(positive_words, min.freq = 1, scale = c(2,1))
barplot(d[5:15,]$freq, las = 2, names.arg = d[5:15,]$word,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")

```

